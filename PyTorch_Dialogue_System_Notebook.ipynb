{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-based Dialogue System on the Ubuntu Dialogue Corpus | PyTorch LSTM \n",
    "\n",
    "This project is a retrieval-based dialogue system, that means a model which classifies whether an utterance is the correct response to a given context utterance or not. Ideally, it is suppose to retrieve the best response to a conversational input from a whole pool of candidate utterances.\n",
    "\n",
    "\n",
    "Please find the necessary data files within this GDrive folder: https://drive.google.com/open?id=1RIIbsS-vxR7Dlo2_v6FWHDFE7q1XPPgj\n",
    "\n",
    "You do not need all of these files, it depends on what sample size you would like to try.\n",
    "--> **To re-run the code as in the presentation and documentation, please do the following:**\n",
    "\n",
    "1) **Download** the following files only:\n",
    "\n",
    "    - glove.6B.50d.txt (Subfolder GloVe)\n",
    "    - training_10000.csv (Subfolder MAIN FILES)\n",
    "    - validation_1000.csv (Subfolder MAIN FILES)\n",
    "    - testing_same_structure_1000.csv (Subfolder MAIN FILES)\n",
    "    - testing_different_structure_100.csv (Subfolder MAIN FILES)\n",
    "    - saved_model_10000_gpu.pt (Subfolder SAVED MODELS)\n",
    "\n",
    "2) **Adjust the variables** for *num_training_examples*, *num_validation_examples*, *embedding_dim*, *test_dataframe_same_structure*, *test_dataframe_different_structure* and the *saved model file name* in the marked places in the code.\n",
    "\n",
    "3) **Adjust the hyperparameter configuration** respectively: in the subfolder SAVED MODELS, along with the model files, you find **screenshots for the models** that tell you the values that were chosen for the hyperparameters in order to achieve the same training & validation results!\n",
    "\n",
    "==========================================================================================================\n",
    "\n",
    "**Important! Please note that:**\n",
    "\n",
    "1) In some places in the notebook, when it comes to choosing the sample sizes for training, validation and testing, it says something like **\"Files included for ... examples\"**. Please **IGNORE** those remarks!\n",
    "\n",
    "2) All of the saved models were trained on a GPU, so if you would like to **test the model**, please **un-comment the \".cuda()\"** in the respective parts of the code!\n",
    "\n",
    "3) In the code as it was submitted, the **hyperparameter for weight_decay** (L2 regularization) was set to the **default value of 0.0**. To define different values for L2 (as needed to re-run the results in the presentation and documentation), it has to be **passed as an argument to the optimizer** in the training method: \n",
    "\n",
    "        def train_model(learning_rate, l2_penalty, epochs): \n",
    "           (...)\n",
    "           optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = learning_rate, weight_decay = l2_penalty)\n",
    "     \n",
    "        train_model(learning_rate = 0.0001, l2_penalty = 0.0001, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.nn import init\n",
    "import torch.nn.utils.rnn \n",
    "import datetime\n",
    "import operator\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining helper functions to create variables needed for training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(csvfile):\n",
    "    dataframe = pd.read_csv(csvfile)\n",
    "    return dataframe\n",
    "\n",
    "def shuffle_dataframe(dataframe):\n",
    "    dataframe.reindex(np.random.permutation(dataframe.index))\n",
    "\n",
    "def create_vocab(dataframe):\n",
    "    vocab = []\n",
    "    word_freq = {}\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        context_cell = row[\"Context\"]\n",
    "        response_cell = row[\"Utterance\"]\n",
    "        \n",
    "        train_words = str(context_cell).split() + str(response_cell).split()\n",
    "        \n",
    "        for word in train_words:\n",
    "          \n",
    "            if word.lower() not in vocab:\n",
    "                vocab.append(word.lower())         \n",
    "                       \n",
    "            if word.lower() not in word_freq:\n",
    "                word_freq[word.lower()] = 1\n",
    "            else:\n",
    "                word_freq[word] += 1\n",
    "    \n",
    "    word_freq_sorted = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "    vocab = [\"<UNK>\"] + [pair[0] for pair in word_freq_sorted]\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def create_word_to_id(vocab):     \n",
    "    enumerate_list = [(id, word) for id, word in enumerate(vocab)]\n",
    "        \n",
    "    word_to_id = {pair[1]: pair[0] for pair in enumerate_list}\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def create_id_to_vec(word_to_id, glovefile): \n",
    "    lines = open(glovefile, 'r').readlines()\n",
    "    id_to_vec = {}\n",
    "    vector = None\n",
    "    \n",
    "    for line in lines:\n",
    "        word = line.split()[0]\n",
    "        vector = np.array(line.split()[1:], dtype='float32') #32\n",
    "        \n",
    "        if word in word_to_id:\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(vector))\n",
    "            \n",
    "    for word, id in word_to_id.items(): \n",
    "        if word_to_id[word] not in id_to_vec:\n",
    "            v = np.zeros(*vector.shape, dtype='float32')\n",
    "            v[:] = np.random.randn(*v.shape)*0.01\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(v))\n",
    "            \n",
    "    embedding_dim = id_to_vec[0].shape[0]\n",
    "    \n",
    "    return id_to_vec, embedding_dim\n",
    "\n",
    "\n",
    "def load_ids_and_labels(row, word_to_id):\n",
    "    context_ids = []\n",
    "    response_ids = []\n",
    "\n",
    "    context_cell = row['Context']\n",
    "    response_cell = row['Utterance']\n",
    "    label_cell = row['Label']\n",
    "\n",
    "    max_context_len = 160\n",
    "    \n",
    "    context_words = context_cell.split()\n",
    "    if len(context_words) > max_context_len:\n",
    "        context_words = context_words[:max_context_len]\n",
    "    for word in context_words:\n",
    "        if word in word_to_id:\n",
    "            context_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            context_ids.append(0) #UNK\n",
    "    \n",
    "    response_words = response_cell.split()\n",
    "    for word in response_words:\n",
    "        if word in word_to_id:\n",
    "            response_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            response_ids.append(0)\n",
    "    \n",
    "    label = np.array(label_cell).astype(np.float32)\n",
    "\n",
    "    return context_ids, response_ids, label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining model classes**\n",
    "\n",
    "*Note: According to PyTorch documentation and forum, the in-built dropout in the LSTM will not apply effectively if num_layers = 1 (since it does by definition not apply to the last layer.) Therefore an additional dropout layer was added and the in-built dropout set to 0.0. See http://pytorch.org/docs/master/nn.html#torch.nn.LSTM and https://discuss.pytorch.org/t/dropout-in-lstm/7784*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            num_layers, \n",
    "            num_directions, \n",
    "            dropout, \n",
    "            bidirectional,\n",
    "            rnn_type,\n",
    "            p_dropout): \n",
    "    \n",
    "            super(Encoder, self).__init__()\n",
    "             \n",
    "            self.input_size = input_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.vocab_size = vocab_size\n",
    "            self.num_layers = num_layers\n",
    "            self.num_directions = num_directions\n",
    "            self.dropout = dropout \n",
    "            self.bidirectional = False\n",
    "            self.rnn_type = 'lstm'\n",
    "            self.p_dropout = p_dropout\n",
    "       \n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.input_size)\n",
    "            self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=False, dropout = dropout, bidirectional=False)\n",
    "            self.dropout_layer = nn.Dropout(self.p_dropout) \n",
    "\n",
    "            self.init_weights()\n",
    "             \n",
    "    def init_weights(self):\n",
    "        init.uniform(self.lstm.weight_ih_l0, a = -0.01, b = 0.01)\n",
    "        init.orthogonal(self.lstm.weight_hh_l0)\n",
    "        self.lstm.weight_ih_l0.requires_grad = True\n",
    "        self.lstm.weight_hh_l0.requires_grad = True\n",
    "        \n",
    "        embedding_weights = torch.FloatTensor(self.vocab_size, self.input_size)\n",
    "            \n",
    "        for id, vec in id_to_vec.items():\n",
    "            embedding_weights[id] = vec\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(embedding_weights, requires_grad = True)\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        outputs, hiddens_tuple = self.lstm(embeddings)\n",
    "        \n",
    "        last_hidden = hiddens_tuple[0] #access first tuple element to get last hidden state, dimensions: (num_layers * num_directions x batch_size x hidden_size)\n",
    "        last_hidden = last_hidden[-1] #access last lstm layer, dimensions: (batch_size x hidden_size)\n",
    "        last_hidden = self.dropout_layer(last_hidden) #dimensions: (batch_size x hidden_size)\n",
    "\n",
    "        return last_hidden\n",
    "\n",
    "    \n",
    "class DualEncoder(nn.Module):\n",
    "     \n",
    "    def __init__(self, encoder):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_size = self.encoder.hidden_size\n",
    "        M = torch.FloatTensor(self.hidden_size, self.hidden_size)     \n",
    "        init.xavier_normal(M)\n",
    "        self.M = nn.Parameter(M, requires_grad = True)\n",
    "\n",
    "    def forward(self, context_tensor, response_tensor):\n",
    "        \n",
    "        context_last_hidden = self.encoder(context_tensor) #dimensions: (batch_size x hidden_size)\n",
    "        response_last_hidden = self.encoder(response_tensor) #dimensions: (batch_size x hidden_size)\n",
    "        \n",
    "        #context = context_last_hidden.mm(self.M).cuda()\n",
    "        context = context_last_hidden.mm(self.M) #dimensions: (batch_size x hidden_size)\n",
    "        context = context.view(-1, 1, self.hidden_size) #dimensions: (batch_size x 1 x hidden_size)\n",
    "        \n",
    "        response = response_last_hidden.view(-1, self.hidden_size, 1) #dimensions: (batch_size x hidden_size x 1)\n",
    "        \n",
    "        #score = torch.bmm(context, response).view(-1, 1).cuda()\n",
    "        score = torch.bmm(context, response).view(-1, 1) #dimensions: (batch_size x 1 x 1) and lastly --> (batch_size x 1)\n",
    "\n",
    "        return score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining method to call all helper functions with desired number of examples and embedding dimension (pretrained embedding vectors taken from GloVe file)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_variables(num_training_examples, num_validation_examples, embedding_dim):\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Creating variables for training and validation...\")\n",
    "\n",
    "    training_dataframe = create_dataframe('training_%d.csv' %num_training_examples)\n",
    "    vocab = create_vocab(training_dataframe)\n",
    "    word_to_id = create_word_to_id(vocab)\n",
    "    id_to_vec, emb_dim = create_id_to_vec(word_to_id, 'glove.6B.%dd.txt' %embedding_dim)\n",
    "\n",
    "    validation_dataframe = create_dataframe('validation_%d.csv' %num_validation_examples)\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Variables created.\\n\")\n",
    "    \n",
    "    return training_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining method to create model instance with desired hyperparameters (we only vary hidden_size and p_dropout)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_model(hidden_size, p_dropout):\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Calling model...\")\n",
    "\n",
    "    encoder = Encoder(\n",
    "            input_size = emb_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            vocab_size = len(vocab),\n",
    "            num_layers = 1,\n",
    "            dropout = 0.0,\n",
    "            num_directions = 1,\n",
    "            bidirectional = False,\n",
    "            rnn_type = 'lstm',\n",
    "            p_dropout = p_dropout)\n",
    "\n",
    "    dual_encoder = DualEncoder(encoder)\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Model created.\\n\")\n",
    "    print(dual_encoder)\n",
    "    \n",
    "    return encoder, dual_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some additional helper methods for training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_count(correct_count, score, label):\n",
    "    if ((score.data[0][0] >= 0.5) and (label.data[0][0] == 1.0)) or ((score.data[0][0] < 0.5) and (label.data[0][0]  == 0.0)):\n",
    "       correct_count +=1  \n",
    "   \n",
    "    return correct_count\n",
    "\n",
    "def get_accuracy(correct_count, dataframe):\n",
    "    accuracy = correct_count/(len(dataframe))\n",
    "        \n",
    "    return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining training and validation method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, l2_penalty, epochs): \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Starting training and validation...\\n\")\n",
    "    print(\"====================Data and Hyperparameter Overview====================\\n\")\n",
    "    print(\"Number of training examples: %d, Number of validation examples: %d\" %(len(training_dataframe), len(validation_dataframe)))\n",
    "    print(\"Learning rate: %.5f, Embedding Dimension: %d, Hidden Size: %d, Dropout: %.2f, L2:%.10f\\n\" %(learning_rate, emb_dim, encoder.hidden_size, encoder.p_dropout, l2_penalty))\n",
    "    print(\"================================Results...==============================\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = learning_rate, weight_decay = l2_penalty)\n",
    "       \n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    #loss_func.cuda()\n",
    "     \n",
    "    best_validation_accuracy = 0.0\n",
    "     \n",
    "    for epoch in range(epochs): \n",
    "                     \n",
    "            shuffle_dataframe(training_dataframe)\n",
    "                        \n",
    "            sum_loss_training = 0.0\n",
    "            \n",
    "            training_correct_count = 0\n",
    "            \n",
    "            dual_encoder.train()\n",
    "\n",
    "            for index, row in training_dataframe.iterrows():            \n",
    "            \n",
    "                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "                \n",
    "                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False) #.cuda()\n",
    "                \n",
    "                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False) #.cuda()\n",
    "                                \n",
    "                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1))), requires_grad = False) #.cuda()\n",
    "                             \n",
    "                score = dual_encoder(context, response)\n",
    "        \n",
    "                loss = loss_func(score, label)\n",
    "                \n",
    "                sum_loss_training += loss.data[0]\n",
    "                \n",
    "                loss.backward()\n",
    "        \n",
    "                optimizer.step()\n",
    "               \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                training_correct_count = increase_count(training_correct_count, score, label)\n",
    "                                                    \n",
    "            training_accuracy = get_accuracy(training_correct_count, training_dataframe)\n",
    "            \n",
    "            #plt.plot(epoch, training_accuracy)\n",
    "                \n",
    "            shuffle_dataframe(validation_dataframe)\n",
    "            \n",
    "            validation_correct_count = 0\n",
    "\n",
    "            sum_loss_validation = 0.0\n",
    "\n",
    "            dual_encoder.eval()\n",
    "\n",
    "            for index, row in validation_dataframe.iterrows():\n",
    "                \n",
    "                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "                \n",
    "                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) #.cuda()\n",
    "                \n",
    "                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) #.cuda()\n",
    "                                \n",
    "                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) #.cuda()\n",
    "                \n",
    "                score = dual_encoder(context, response)\n",
    "                \n",
    "                loss = loss_func(score, label)\n",
    "                \n",
    "                sum_loss_validation += loss.data[0]\n",
    "                \n",
    "                validation_correct_count = increase_count(validation_correct_count, score, label)\n",
    "                    \n",
    "            validation_accuracy = get_accuracy(validation_correct_count, validation_dataframe)\n",
    "                        \n",
    "            print(str(datetime.datetime.now()).split('.')[0], \n",
    "                  \"Epoch: %d/%d\" %(epoch,epochs),  \n",
    "                  \"TrainLoss: %.3f\" %(sum_loss_training/len(training_dataframe)), \n",
    "                  \"TrainAccuracy: %.3f\" %(training_accuracy), \n",
    "                  \"ValLoss: %.3f\" %(sum_loss_validation/len(validation_dataframe)), \n",
    "                  \"ValAccuracy: %.3f\" %(validation_accuracy))\n",
    "            \n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                torch.save(dual_encoder.state_dict(), 'saved_model_%d_examples.pt' %(len(training_dataframe)))\n",
    "                print(\"New best found and saved.\")\n",
    "                \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Training and validation epochs finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing data size and embedding dimension, creating variables needed for training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-02 20:55:08 Creating variables for training and validation...\n",
      "2018-02-02 20:55:16 Variables created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe = creating_variables(num_training_examples = 10000, \n",
    "                                                                                                     embedding_dim = 50, \n",
    "                                                                                                     num_validation_examples = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing hidden size and dropout probability, creating model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-02 20:55:16 Calling model...\n",
      "2018-02-02 20:55:16 Model created.\n",
      "\n",
      "DualEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(6885, 50)\n",
      "    (lstm): LSTM(50, 50)\n",
      "    (dropout_layer): Dropout(p=0.75)\n",
      "  )\n",
      ")\n",
      "M\n",
      "encoder.embedding.weight\n",
      "encoder.lstm.weight_ih_l0\n",
      "encoder.lstm.weight_hh_l0\n",
      "encoder.lstm.bias_ih_l0\n",
      "encoder.lstm.bias_hh_l0\n"
     ]
    }
   ],
   "source": [
    "encoder, dual_encoder = creating_model(hidden_size = 50, \n",
    "                                       p_dropout = 0.85)\n",
    "\n",
    "#encoder.cuda()\n",
    "#dual_encoder.cuda\n",
    "\n",
    "for name, param in dual_encoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing learning rate and number of epochs, starting training and validation epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-02 20:55:17 Starting training and validation...\n",
      "\n",
      "====================Data and Hyperparameter Overview====================\n",
      "\n",
      "Number of training examples: 1000, Number of validation examples: 100\n",
      "Learning rate: 0.00010, Embedding Dimension: 50, Hidden Size: 50, Dropout: 0.75, L2:0.0001000000\n",
      "\n",
      "================================Results...==============================\n",
      "\n",
      "2018-02-02 20:55:50 Epoch: 0/100 TrainLoss: 0.693 TrainAccuracy: 0.496 ValLoss: 0.693 ValAccuracy: 0.540\n",
      "New best found and saved.\n",
      "2018-02-02 20:56:26 Epoch: 1/100 TrainLoss: 0.694 TrainAccuracy: 0.496 ValLoss: 0.693 ValAccuracy: 0.540\n",
      "2018-02-02 20:57:08 Epoch: 2/100 TrainLoss: 0.695 TrainAccuracy: 0.496 ValLoss: 0.693 ValAccuracy: 0.540\n",
      "2018-02-02 20:57:50 Epoch: 3/100 TrainLoss: 0.695 TrainAccuracy: 0.496 ValLoss: 0.694 ValAccuracy: 0.540\n",
      "2018-02-02 20:58:31 Epoch: 4/100 TrainLoss: 0.696 TrainAccuracy: 0.496 ValLoss: 0.694 ValAccuracy: 0.540\n",
      "2018-02-02 20:59:11 Epoch: 5/100 TrainLoss: 0.693 TrainAccuracy: 0.496 ValLoss: 0.694 ValAccuracy: 0.540\n",
      "2018-02-02 20:59:50 Epoch: 6/100 TrainLoss: 0.691 TrainAccuracy: 0.496 ValLoss: 0.693 ValAccuracy: 0.540\n",
      "2018-02-02 21:00:30 Epoch: 7/100 TrainLoss: 0.694 TrainAccuracy: 0.496 ValLoss: 0.694 ValAccuracy: 0.540\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fb3014ba3ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model(learning_rate = 0.0001, \n\u001b[1;32m      2\u001b[0m             \u001b[0ml2_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             epochs = 100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-14616402315e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(learning_rate, l2_penalty, epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(learning_rate = 0.0001, \n",
    "            l2_penalty = 0.0001,\n",
    "            epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading saved model for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_encoder.load_state_dict(torch.load('saved_model_1000_examples.pt'))\n",
    "\n",
    "dual_encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing approach 1:**\n",
    "\n",
    "*Test data has same structure as training and validation data (context, response, label)*\n",
    "\n",
    "*Test metric: Accuracy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data (files included for number of testing examples = 100, 50, 20, 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe_same_structure = pd.read_csv('testing_same_structure_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining method to compute scores and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_same_structure():\n",
    "    \n",
    "    test_correct_count = 0\n",
    "\n",
    "    for index, row in test_dataframe_same_structure.iterrows():\n",
    "\n",
    "        context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "\n",
    "        context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) #.cuda()\n",
    "\n",
    "        response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) #.cuda()\n",
    "\n",
    "        label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) #.cuda()\n",
    "\n",
    "        score = dual_encoder(context, response)\n",
    "\n",
    "        test_correct_count = increase_count(test_correct_count, score, label)\n",
    "\n",
    "    test_accuracy = get_accuracy(test_correct_count, test_dataframe_same_structure)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = testing_same_structure()\n",
    "print(\"Test accuracy for %d training examples and %d test examples: %.2f\" %(len(training_dataframe),len(test_dataframe_same_structure),test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing approach 2**\n",
    "\n",
    "*Test data has different structure than training and validation data (1 context utterance, 1 ground truth utterance, 9 distractor utterances)*\n",
    "\n",
    "*Test metric: Recall*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data (files included for number of testing examples = 20, 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe_different_structure = pd.read_csv('testing_different_structure_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining method to store each example's word IDs per utterance in a dictionary:\n",
    "\n",
    "*Outer dictionary \"ids_per_example_and_candidate\": keys = examples, values = inner dictionaries*\n",
    "\n",
    "*Inner dictionaries \"ids_per_candidate\": keys = candidate names, values = list of word IDs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ids(test_dataframe_different_structure, word_to_id):\n",
    "    \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Loading test IDs...\")\n",
    "\n",
    "    max_context_len = 160\n",
    "    \n",
    "    ids_per_example_and_candidate = {}\n",
    "    \n",
    "    for i, example in test_dataframe_different_structure.iterrows():\n",
    "        \n",
    "        ids_per_candidate = {}\n",
    "      \n",
    "        for column_name, cell in  example.iteritems():\n",
    "            \n",
    "                id_list = []\n",
    "            \n",
    "                words = str(cell).split()\n",
    "                if len(words) > max_context_len:\n",
    "                    words = words[:max_context_len]\n",
    "    \n",
    "                for word in words:\n",
    "                    if word in word_to_id:\n",
    "                        id_list.append(word_to_id[word])\n",
    "                    else: \n",
    "                        id_list.append(0) #UNK  \n",
    "                    \n",
    "                ids_per_candidate[column_name] = id_list\n",
    "    \n",
    "        ids_per_example_and_candidate[i] = ids_per_candidate\n",
    "    \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Test IDs loaded.\")\n",
    "    \n",
    "    return ids_per_example_and_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_per_example_and_candidate = load_ids(test_dataframe_different_structure, word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining method to store each example's scores per utterance in a dictionary: (in analogy to above)\n",
    "\n",
    "*Outer dictionary \"scores_per_example_and_candidate\": keys = examples, values = inner dictionaries*\n",
    "\n",
    "*Inner dictionaries \"scores_per_candidate\": keys = candidate names, values = score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(): \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Computing test scores...\")\n",
    "    \n",
    "    scores_per_example_and_candidate = {}\n",
    "                 \n",
    "    for example, utterance_ids_dict in sorted(ids_per_example_and_candidate.items()): \n",
    "        \n",
    "        score_per_candidate = {}\n",
    "\n",
    "        for utterance_name, ids_list in sorted(utterance_ids_dict.items()):\n",
    "        \n",
    "            context = autograd.Variable(torch.LongTensor(utterance_ids_dict['Context']).view(-1,1))#.cuda()\n",
    "            \n",
    "            if utterance_name != 'Context':\n",
    "\n",
    "                candidate_response = autograd.Variable(torch.LongTensor(utterance_ids_dict[utterance_name]).view(-1, 1))#.cuda()\n",
    "                        \n",
    "                score = torch.sigmoid(dual_encoder(context, candidate_response))\n",
    "                \n",
    "                score_per_candidate[\"Score with \" + utterance_name] = score.data[0][0]\n",
    "    \n",
    "        scores_per_example_and_candidate[example] = score_per_candidate\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Test scores computed.\")\n",
    "    \n",
    "    return scores_per_example_and_candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_example_and_candidate = load_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining method to compute results for recall metric: \n",
    "\n",
    "*If 1 in 10 utterances (1 ground truth + 9 distractors) is the correct response to a context utterance, what's the chance that the ground truth utterance is among the utterances with the k = 5,2,1 highest scores?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_at_k(k):\n",
    "    count_true_hits = 0\n",
    "    \n",
    "    for example, score_per_candidate_dict in sorted(scores_per_example_and_candidate.items()): \n",
    "    \n",
    "        top_k = dict(sorted(score_per_candidate_dict.items(), key=operator.itemgetter(1), reverse=True)[:k])\n",
    "        \n",
    "        if 'Score with Ground Truth Utterance' in top_k:\n",
    "            count_true_hits += 1\n",
    "    \n",
    "    number_of_examples = len(scores_per_example_and_candidate)\n",
    "    \n",
    "    recall_at_k = count_true_hits/number_of_examples\n",
    "    \n",
    "    return recall_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"recall_at_5 =\",get_recall_at_k(k = 5)) #Baseline expectation: 5/10 = 0.5 for random guess\n",
    "print(\"recall_at_2 =\",get_recall_at_k(k = 2)) #Baseline expectation: 2/10 = 0.2 for random guess\n",
    "print(\"recall_at_1 =\",get_recall_at_k(k = 1)) #Baseline expectation: 1/10 = 0.1 for random guess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
